Then create a development configuration file at `configuration/dev.properties` with your Confluent Cloud information (specifically fill in the `bootstrap.servers`, `sasl.jaas.config`, `basic.auth.user.info`, and `schema.registry.url` configs):

```
# General cluster connection information
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
bootstrap.servers=<BROKER ENDPOINT>
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='<API KEY>' password='<API SECRET>';
basic.auth.credentials.source=USER_INFO
basic.auth.user.info=<SR API KEY>:<SR API SECRET>
schema.registry.url=https://<SR ENDPOINT>

# Consumer properties
key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
max.poll.interval.ms=300000
enable.auto.commit=true
auto.offset.reset=earliest
group.id=consumer-application

# Application specific properties
file.path=consumer-records.out
input.topic.name=input-topic
input.topic.partitions=6
input.topic.replication.factor=3
```

Let's do a quick overview of some of the more important properties here:

The `key.deserializer` and `value.deserializer` properties provide a class implementing the `Deserializer` interface for converting `byte` arrays into the expected object type of the key and value respectively.

The `max.poll.interval.ms` is the maximum amount of time a consumer may take between calls to `Consumer.poll()`.  If a consumer instance takes longer than the specified time, it's considered non-responsive and removed from the consumer-group triggering a rebalance.

Setting `enable.auto.commit` configuration to `true` enables the Kafka consumer to handle committing offsets automatically for you.  The default setting is `true`, but it's included here to make it explicit.  When you enable auto commit, you need to ensure you've processed all records _**before**_ the consumer calls `poll` again.  Once there is a subsequent call to `poll`, all the records returned from the previous call are considered processed and the consumer commits the offsets.

`auto.offset.reset` - If a consumer instance can't locate any offsets for its topic-partition assignment(s), it will resume processing from the _**earliest**_ available offset.

`group.id` - Kafka uses the concept of a consumer-group which is used to represent a logical single group.  A consumer-group can be made up of multiple members all sharing the same `group.id` configuration.  As members leave or join the consumer-group, the group-coordinator triggers a rebalance which causes topic-partition reassignment among active members of the group.
